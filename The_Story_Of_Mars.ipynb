{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Daniel Gallagher, 18401492\n",
    "\n",
    "Data: NASA's Mars InSight Weather Data\n",
    "\n",
    "# Need to include information about Sols, Mars Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from Red Skies\n",
    "We haven't long been in this liminal state, pushing towards space exploration and discovery since only the last century. Desperate to escape the restrictions of life on Earth, we wish to go further than our *“pale blue dot”*. We've oft travelled towards things we felt bigger than ourselves barefoot, by sails, and on wheels. We now go a step further than that faithful day in July 1969 and look towards a red sky.\n",
    "\n",
    "In 2011, NASA began the concept design for a mission to retrieve huge amounts of data from the rough surface of Mars. It would finally paint a picture for us of the deeply rich history of Mars' geological evolution. The data that we will be looking at was worth the cost of $830 million to NASA, and had many difficulties in its 7 year development.\n",
    "\n",
    "This robotic lander was known as **InSight**.\n",
    "\n",
    "![The InSight Lander on Mars](./images/lander_on_mars.png)\n",
    "\n",
    "# Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (or, uhm, InSight)\n",
    "\n",
    "What has shaped this rocky planet of the interior Solar System over the last four billion years? This was the question on the minds of those designing each and every small detail on the Mars InSight robotic lander. It influenced every decision they made, and hence formed the attributes of the data we would receive from the red planet.\n",
    "\n",
    "We will be examining data from a suite of sensors installed on the lander known as the [TWINS][1]. \n",
    "\n",
    "These sensors include:\n",
    "* **Thermometers** for collecting atmospheric temperature information\n",
    "* **An anemometer** for collecting wind speed and direction information\n",
    "* **A highly sensitive barometer** for collecting atmospheric pressure information\n",
    "\n",
    "Together, these tools will allow us to shape a fascinating picture of the Mars' climate and how it relates to our own.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Temperature_and_Winds_for_InSight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Retrieving & Parsing InSight Data\n",
    "\n",
    "We begin our data retrieval with requesting access to it. NASA allows us access through an **API key** which I signed up for on their website. Additionally, I have decided to use the [requests][1] library in order to retrieve this data from their server.\n",
    "\n",
    "Let's begin by importing this library and specifying these important parameters for interaction with NASA.\n",
    "[1]: https://requests.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#URL & Parameters \n",
    "url = 'https://api.nasa.gov/insight_weather/'\n",
    "api_key = \"fkkjfgveRAJ2BOVq7gaUAbBbM8omgKo0IRaDEGTj\"\n",
    "feedtype = \"json\"\n",
    "ver = \"1.0\"\n",
    "\n",
    "#Store these parameters in their own dictionary\n",
    "params = dict(api_key=api_key, feedtype=feedtype, ver=ver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure that our request was successful, we will implement a function that checks the **status code** is in order. Thankfully, this is made easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_successful(response):\n",
    "\treturn response.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a call to the server and request the InSight data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Successful: True\n"
     ]
    }
   ],
   "source": [
    "#Send out our request and save the response\n",
    "resp = requests.get(url=url, params=params)\n",
    "\n",
    "#Ensure the response is what we're looking for\n",
    "request_successful = is_successful(resp)\n",
    "print (\"Request Successful: %s\" % (request_successful))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perfect! We've successfully retrieved **something** from NASA.\n",
    "\n",
    "However, that's about all we know at the moment. We need to explore the response further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print \"Response Type: %s\" % (type(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Its a response! Wait... we already knew that...\n",
    "Maybe we can explore this **Response** class and find out more about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://api.nasa.gov/insight_weather/?ver=1.0&api_key=fkkjfgveRAJ2BOVq7gaUAbBbM8omgKo0IRaDEGTj&feedtype=json \n",
      "\n",
      "Encoding: utf-8 \n",
      "\n",
      "Time Elapsed: 0:00:00.598070 \n",
      "\n",
      "Beginning Of Data:\n",
      "{\n",
      "  \"684\": {\n",
      "    \"AT\": {\n",
      "      \"av\": -64.245, \n",
      "      \"ct\": 169658, \n",
      "      \"mn\": -95.453, \n",
      "      \"mx\": -17.861\n",
      "    }, \n",
      "    \"First_UTC\": \"2020-10-29T00:28:37Z\", \n",
      "    \"Last_UTC\": \"2020-10-30T01:08:12Z\", \n"
     ]
    }
   ],
   "source": [
    "#The full URL that we retrieved the data from\n",
    "url = resp.url\n",
    "\n",
    "#The encoding of the information sent to us\n",
    "enc = resp.encoding\n",
    "\n",
    "#The time from first sending the request to receiving a response\n",
    "time_elapsed = resp.elapsed\n",
    "\n",
    "#The beginning of the data we've received \n",
    "start_of_data = resp.text[:200]\n",
    "\n",
    "print (\"URL: %s \\n\\nEncoding: %s \\n\\nTime Elapsed: %s \\n\\nBeginning Of Data:\\n%s\" %\n",
    "      (url, enc, time_elapsed, start_of_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! We've learnt a good deal more about what we're working with.\n",
    "\n",
    "Examine for a moment the beginning of the data. It gives us an important insight about the data; it is clearly in a **JSON format**. \n",
    "Our response class is excellent for handling this format, and we can thus easily create a dictionary object from this JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "data = resp.json()\n",
    "print (\"Type: %s\" % (type(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are entering the final step of our data retrieval & parsing process.\n",
    "\n",
    "We would like to store this data in a JSON file for future use. \n",
    "Firstly, I'll create a function which will write response information to a file we pass in.\n",
    "Our response class has an iterator object available through **iter_content** which will be useful here.\n",
    "\n",
    "Next, I'll pass in where I would like to store this JSON file and run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json_file(response, to_file):\n",
    "\twith open(to_file, \"w\") as f:\n",
    "\t\tfor chunk in response.iter_content(chunk_size=128):\n",
    "\t\t\tf.write(chunk) \n",
    "\t\tf.close()\n",
    "        \n",
    "json_file = \"./json_storage/insight_data_storage.json\"\n",
    "save_to_json_file(resp, json_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion On Data Retrieval & Parsing\n",
    "We began with a simple request out to NASA's InSight server and no knowledge about what we had received or if the request was successful.\n",
    "\n",
    "We then ensured that NASA had responded to our request correctly. From this, we built up a knowledge of the type of data we had received and stored it in two separate ways based on this knowledge:\n",
    "* We learned that it was given to us as JSON, and thus saved it to a python dictionary.\n",
    "* We then saved the information to a JSON file for future use.\n",
    "\n",
    "This concludes the process of data retrieval and parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing & Integration\n",
    "\n",
    "Data preprocessing is an incredibly important step in any data scientist's workflow. \n",
    "To have datasets which are poorly managed and cared for is to deal with a whole array of problems down the line. \n",
    "\n",
    "We will handle this step in three different stages:\n",
    "* Data Cleaning & Reduction\n",
    "* Data Transformation\n",
    "\n",
    "When cultivated, these steps will lead to data from InSight that can be used confidently and efficiently.\n",
    "\n",
    "We will use data saved from a previous call to NASA's server. This will be loaded into our **data** dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "previously_stored_json_path = \"./json_storage/previously_stored_mars_data.json\"\n",
    "\n",
    "with open(previously_stored_json_path) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally have a look at the attributes of our data.\n",
    "We can do this by examining each **key** in our **data** dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute 1: 674\n",
      "Attribute 2: 675\n",
      "Attribute 3: 676\n",
      "Attribute 4: 670\n",
      "Attribute 5: 671\n",
      "Attribute 6: 672\n",
      "Attribute 7: 673\n",
      "Attribute 8: validity_checks\n",
      "Attribute 9: sol_keys\n"
     ]
    }
   ],
   "source": [
    "data_attributes = []\n",
    "\n",
    "for i,attr in enumerate(data):\n",
    "    data_attributes.append(attr)\n",
    "    print (\"Attribute %d: %s\" % (i+1,attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, cool! We have some attributes. But what do they mean?\n",
    "\n",
    "We can understand these attributes as:\n",
    "* The previous seven sols (1 Mars Day = 1 Sol) \n",
    "* Ensuring we can be confident in the data through validity checks\n",
    "* A list of each of the seven sol keys\n",
    "\n",
    "However, none of these attributes tell us about our sensors! \n",
    "We want to see some barometer action.\n",
    "\n",
    "Let's look inside a single Sol and see what attributes it presents to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sol: 670\n",
      "\n",
      "Attribute 1: PRE\n",
      "Attribute 2: WD\n",
      "Attribute 3: Season\n",
      "Attribute 4: Last_UTC\n",
      "Attribute 5: AT\n",
      "Attribute 6: First_UTC\n",
      "Attribute 7: HWS\n"
     ]
    }
   ],
   "source": [
    "#First, we'll save all Sols into a list\n",
    "sols = data[\"sol_keys\"]\n",
    "\n",
    "#Choose the first sol as our example\n",
    "example_sol = sols[0]\n",
    "example_sol_data = data[example_sol]\n",
    "\n",
    "print(\"Example Sol: %s\\n\" % (example_sol))\n",
    "\n",
    "sol_attributes = []\n",
    "for i,attr in enumerate(example_sol_data):\n",
    "    sol_attributes.append(attr)\n",
    "    print (\"Attribute %d: %s\" % (i+1,attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the aforementioned **TWINS sensor suite**:\n",
    "* **PRE** - A barometer which feeds us our **atmospheric pressure** data.\n",
    "* **WD & HWS** - An Anemometer which tells us about **wind direction and speed**, respectively.\n",
    "* **AT** - Multiple thermometers which tell us about the **atmospheric temperature**.\n",
    "* **Season** - Tells us what season it is on Mars.\n",
    "* **First/Last UTC** - The time of the first and last datum transmitted on any given sol.\n",
    "\n",
    "We now have a good idea of the important attributes within our dataset. This will guide us in the data cleansing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Data cleaning is the process of removing any unwanted data from our dataset.\n",
    "This includes errors and irrelevant information.\n",
    "The NASA data received is very well kept, however we must ensure that there are no faults.\n",
    "\n",
    "We will clean / scrub our data in these steps:\n",
    "* Remove Unreliable/Missing Data\n",
    "* Remove Irrelevant Information\n",
    "* Quality Assurance\n",
    "\n",
    "### Removing Unreliable / Missing Data Points\n",
    "They say that the devil is in the sensors, or is it the details..? Either way, **we can be confident in our data if and only if we can be confident in our sensors**. That is, the barometer, anemometer, and thermometers.\n",
    "\n",
    "NASA has made this very simple for us. Included in the data we received is an attribute called **validity_checks**. This attributes ensures our data is accurate by telling us how many data points were received by each sensor on each Sol (day on Mars). \n",
    "\n",
    "Let's examine the attributes of these validity checks for our sensors in our example sol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Validity Attributes Of PRE\n",
      "Attribute: valid\n",
      "Attribute: sol_hours_with_data\n",
      "\n",
      "\n",
      "Inspecting Validity Attributes Of AT\n",
      "Attribute: valid\n",
      "Attribute: sol_hours_with_data\n",
      "\n",
      "\n",
      "Inspecting Validity Attributes Of HWS\n",
      "Attribute: valid\n",
      "Attribute: sol_hours_with_data\n",
      "\n",
      "\n",
      "Inspecting Validity Attributes Of WD\n",
      "Attribute: valid\n",
      "Attribute: sol_hours_with_data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Store our validity data\n",
    "validity_data = data[\"validity_checks\"]\n",
    "\n",
    "#Example Sol's Validity Checks \n",
    "example_validity = validity_data[example_sol]\n",
    "\n",
    "#We only require validation for the sensors onboard.\n",
    "sensors = [\"PRE\",\"AT\",\"HWS\",\"WD\"]\n",
    "\n",
    "#Iterate Through The Validity Checks Of Each Sensor\n",
    "for i,sensor in enumerate(sensors):\n",
    "    print (\"Inspecting Validity Attributes Of %s\" % (sensor))\n",
    "    \n",
    "    for attr in example_validity[sensor]:\n",
    "        print (\"Attribute: %s\" % (attr))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our sensors, we can see NASA has included a **valid** boolean attribute in the **validity checks**. \n",
    "This attribute will equal **\"True\" if there are at least 18 hours on a given martian day with data sent from a sensor**. If there are less, it will equal \"False\".\n",
    "\n",
    "To summarise, this validity check ensures:\n",
    "* Data is present and not missing\n",
    "* There is enough data to be considered reliable\n",
    "\n",
    "This is an excellent example of **feature construction**. The InSight lander sent the data and NASA constructed a validity variable based on the consistency/reliability of these data. They then construct each of the variables we're working with in each sol around these data points, removing the possibility of missing data.\n",
    "\n",
    "We can now validate each sensor by ensuring these **valid** attributes are true on each sol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Sol: 670\n",
      "Validity of PRE For Sol 670: True\n",
      "Validity of AT For Sol 670: True\n",
      "Validity of HWS For Sol 670: True\n",
      "Validity of WD For Sol 670: True\n",
      "\n",
      "Next Sol: 671\n",
      "Validity of PRE For Sol 671: True\n",
      "Validity of AT For Sol 671: True\n",
      "Validity of HWS For Sol 671: True\n",
      "Validity of WD For Sol 671: True\n",
      "\n",
      "Next Sol: 672\n",
      "Validity of PRE For Sol 672: True\n",
      "Validity of AT For Sol 672: True\n",
      "Validity of HWS For Sol 672: True\n",
      "Validity of WD For Sol 672: True\n",
      "\n",
      "Next Sol: 673\n",
      "Validity of PRE For Sol 673: True\n",
      "Validity of AT For Sol 673: True\n",
      "Validity of HWS For Sol 673: True\n",
      "Validity of WD For Sol 673: True\n",
      "\n",
      "Next Sol: 674\n",
      "Validity of PRE For Sol 674: True\n",
      "Validity of AT For Sol 674: True\n",
      "Validity of HWS For Sol 674: True\n",
      "Validity of WD For Sol 674: True\n",
      "\n",
      "Next Sol: 675\n",
      "Validity of PRE For Sol 675: True\n",
      "Validity of AT For Sol 675: True\n",
      "Validity of HWS For Sol 675: True\n",
      "Validity of WD For Sol 675: True\n",
      "\n",
      "Next Sol: 676\n",
      "Validity of PRE For Sol 676: True\n",
      "Validity of AT For Sol 676: True\n",
      "Validity of HWS For Sol 676: True\n",
      "Validity of WD For Sol 676: True\n"
     ]
    }
   ],
   "source": [
    "for sol in sols:\n",
    "    validity = validity_data[sol]\n",
    "    \n",
    "    print(\"\\nNext Sol: %s\" % (sol))\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        is_valid = validity[sensor][\"valid\"]\n",
    "        \n",
    "        print(\"Validity of %s For Sol %s: %s\" % (sensor,sol, is_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, all our sensors are working and sending enough information to be considered reliable. They didn't spend hundreds of millions for nothing! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Irrelevant Information\n",
    "\n",
    "We can begin removing data that we don't want from our dataset. \n",
    "\n",
    "To begin, let's examine again the various attributes that we've been working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Attributes\n",
      "674\n",
      "675\n",
      "676\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "validity_checks\n",
      "sol_keys\n",
      "\n",
      "\n",
      "Individual Sol Attributes\n",
      "PRE\n",
      "WD\n",
      "Season\n",
      "Last_UTC\n",
      "AT\n",
      "First_UTC\n",
      "HWS\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Attributes\")\n",
    "for attr in data_attributes:\n",
    "    print(attr)\n",
    "print('\\n')\n",
    "    \n",
    "print(\"Individual Sol Attributes\")\n",
    "for attr in sol_attributes:\n",
    "    print (attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubt there's a lot of interesting information here, but there's a difference between interesting and useful.\n",
    "What can we prune away?\n",
    "\n",
    "#### Pruning Data Attributes\n",
    "Looking towards our data attributes, we definitely want to keep each individual sol. However, the validity checks are no longer useful as we have validated our data. These checks are purely for developers and are of no interest in extracting insights from our data further down the pipeline.\n",
    "\n",
    "#### Pruning Sol Attributes\n",
    "We would like to keep most of the attributes in each sol, as this is where we can really extract understanding about the nature of Mars' climate. However, we can remove the attributes which tell us the time of the first datum and last datum. This is perhaps interesting data, but not useful.\n",
    "\n",
    "There is also the most_common attribute in each wind direction dataset. We want the all of the data wind direction data rather than just the most common direction during the day, therefore we will remove this attribute.\n",
    "\n",
    "\n",
    "#### Final Prune\n",
    "We have decided to remove the following unnecessary attributes from our dataset:\n",
    "* Data - Validity Checks\n",
    "* Sol  - First UTC\n",
    "* Sol  - Last UTC\n",
    "* Wind Dir - Most Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Validity Checks & Sol Keys\n",
    "data.pop(\"validity_checks\", None)\n",
    "\n",
    "#Remove Time Information\n",
    "for sol in sols:\n",
    "    data[sol].pop(\"First_UTC\", None)\n",
    "    data[sol].pop(\"Last_UTC\", None)\n",
    "    \n",
    "    data[sol][\"WD\"].pop(\"most_common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assurance\n",
    "\n",
    "Quality assurance involves making sure that our cleansing of the data has been successful. \n",
    "We will do this by ensuring that we have succesfully structured our dataset as we wanted in the previous example.\n",
    "\n",
    "We will begin by ensuring that we removed the \"validity_checks\" attribute from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity Checks Removed: True\n"
     ]
    }
   ],
   "source": [
    "#Ensure an attribute is no longer in our dataset\n",
    "def is_removed(dataset, attr):\n",
    "    return attr not in dataset\n",
    "\n",
    "is_validity_removed = is_removed(data, \"validity_checks\")\n",
    "\n",
    "print(\"Validity Checks Removed: %s\" % (is_validity_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully removed these unnecessary attributes.\n",
    "\n",
    "Next, we want to ensure that we are only left with the sensors and the season attribute in each sol. Let's examine each sol to ensure this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sol 670 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n",
      "\n",
      "Sol 671 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n",
      "\n",
      "Sol 672 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n",
      "\n",
      "Sol 673 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n",
      "\n",
      "Sol 674 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n",
      "\n",
      "Sol 675 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n",
      "\n",
      "Sol 676 Attributes: [u'PRE', u'WD', u'Season', u'AT', u'HWS']\n",
      "Most Common Removed: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sol in sols:\n",
    "    print (\"\\nSol %s Attributes: %s\" % (sol, data[sol].keys()))\n",
    "    print (\"Most Common Removed: %s\\n\" % (\"most_common\" not in data[sol][\"WD\"].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also successfully removed unnecessary attributes here.\n",
    "\n",
    "This concludes our quality assurance for the dataset, and thus we have completed the data cleansing process. We can now be confident in the reliability and consistency of our data.\n",
    "\n",
    "However, can we improve the structure of our data? Are we working with good database structure principles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation \n",
    "\n",
    "Data transformation handles the way data are stored, maintained, and restrieved. We will be implementing a relational database management systems (RDBMS). Following certain rules, this will ensure good data management practices can be upheld.\n",
    "\n",
    "In order to begin the restructuring of our data, we must implement a consistent procedure for accessing our data.\n",
    "Accessing each table through a dictionary worked previously, but is not optimal.\n",
    "\n",
    "We will begin by setting up an enum class that we can reference to find an ID for each sensor, rather than writing the string each time. This follows a good programming principle where nothing is \"working only by coincedence\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An enum to reference which table we're referring to\n",
    "class Table(enum.Enum):\n",
    "    Sols = 0\n",
    "    Temp = 1\n",
    "    W_Speed = 2\n",
    "    W_Dir = 3\n",
    "    Pressure = 4\n",
    "    \n",
    "table_ref = {\n",
    "    Table.Sols : \"Sols\",\n",
    "    Table.Temp : \"AT\",\n",
    "    Table.W_Speed : \"HWS\",\n",
    "    Table.W_Dir : \"WD\",\n",
    "    Table.Pressure : \"PRE\"\n",
    "}\n",
    "\n",
    "def get_tbl_id(table):\n",
    "    return table_ref[table]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to begin setting up a new database.\n",
    "We're going to use a mixture SQLite & Pandas in order to create a new database located in the \"db\" folder in our project.\n",
    "\n",
    "We first want to create some functions that will allow us to very easily work with SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "def connect_to_db(name):\n",
    "    return sql.connect(\"db/\" + table_name)\n",
    "\n",
    "#Save a table to the relational database\n",
    "def save_data(name, rows, columns):\n",
    "    db_conn = connect_to_db(name)\n",
    "    df = pd.DataFrame(data=rows, columns=columns)\n",
    "    df.to_sql(name,db_conn,index=False,if_exists='replace')\n",
    "    db_conn.close()\n",
    "\n",
    "#Retrieve an entire table from the database\n",
    "def get_table(tbl):\n",
    "    db_conn = connect_to_db(tbl)\n",
    "    sql_cmd = \"SELECT * FROM %s\" % (tbl)\n",
    "    df = pd.read_sql(sql_cmd,db_conn)\n",
    "    db_conn.close()\n",
    "    return df\n",
    "\n",
    "#Execute a custom SQL query on a table\n",
    "def query_table(tbl,query):\n",
    "    db_conn = connect_to_db(tbl)\n",
    "    cur = db_conn.cursor()\n",
    "    exe = cur.execute(query)\n",
    "    rows = [row for row in exe]\n",
    "    db_conn.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that we create proper relations between different tables and follow good relational database practices.\n",
    "\n",
    "Below, I have laid out the structure that I will be setting up for my data in this project.\n",
    "There is a table for each sensor and any additional tables required for making relations between tables.\n",
    "We can see that there is one-to-many relationship for our sols. Note each of the unique keys for our table. \n",
    "\n",
    "Originally I wondered if it would be suitable to set up a table for ordinals on the compass, to give them a proper ID. However, the number of the ordinal is already the perfect ID and there is no other information required. Therefore, a table was not created to store IDs for ordinal numbers.\n",
    "\n",
    "![Relational Database Model For Mars Data](./images/db_structure.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Time! Setting up the Sols table\n",
    "\n",
    "Let's have another look at the current attributes of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['674', '675', '676', '670', '671', '672', '673', 'sol_keys']\n"
     ]
    }
   ],
   "source": [
    "print ([str(x) for x in data.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data is currently grouped by sol.\n",
    "\n",
    "This is not an optimal implementation. We want to create a unique sol ID for each sol and store it in its own table along with the particular day of the year as we see above. This table will then be used for a one-to-many relationship with other tables grouped by subject.\n",
    "\n",
    "Let us begin with our new table for sols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sol_id</th>\n",
       "      <th>days_into_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sol_id days_into_year\n",
       "0       0            670\n",
       "1       1            671\n",
       "2       2            672\n",
       "3       3            673\n",
       "4       4            674\n",
       "5       5            675\n",
       "6       6            676"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sols Table\n",
    "Name: SOLS\n",
    "2 Attributes: Unique Sol ID | Number of Days into the Year\n",
    "\"\"\"\n",
    "table_name = get_tbl_id(Table.Sols)\n",
    "\n",
    "#The data we're already given about sols\n",
    "days_into_year = data[\"sol_keys\"]\n",
    "\n",
    "#Unique Sol ID: Starting from 0\n",
    "unique_ids = [i for i in range(0,len(days_into_year))]\n",
    "\n",
    "#Attach our two rows together\n",
    "rows = zip(unique_ids, days_into_year)\n",
    "\n",
    "#Create the columns\n",
    "cols = [\"sol_id\", \"days_into_year\"]\n",
    "\n",
    "#Save our data to our relational database\n",
    "save_data(table_name, rows, cols)\n",
    "\n",
    "get_table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above table will store the IDs we need for creating primary keys in the tables for each sensor.\n",
    "We can create a few functions which will make the process of table creation much easier for the sensors.\n",
    "\n",
    "The function **structure_sensor_data** will accept a sensor ID and create a table of everything that sensor picked up over the last seven days on Mars. Our other function will convert from *sol id* to *day of the year*. \n",
    "\n",
    "For example, ID 0 => Day 612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last seven days on Mars\n",
    "recent_sols = get_table(get_tbl_id(Table.Sols))[\"sol_id\"]\n",
    "\n",
    "#Convert Sol ID => Day Of The Year\n",
    "def id_to_sol(sol_id):\n",
    "    query = \"SELECT days_into_year FROM Sols WHERE sol_id=%s\" % (sol_id)\n",
    "    tbl = \"Sols\"\n",
    "    return str(query_table(tbl,query)[0][0])\n",
    "\n",
    "def structure_sensor_data(sensor_id, recent_sols):\n",
    "    sensor_rows = []\n",
    "    sensor_cols = [\"sol_id\", \"av\", \"mn\", \"mx\", \"ct\"]\n",
    "    \n",
    "    for sol_id in recent_sols:\n",
    "        day = id_to_sol(sol_id)\n",
    "        \n",
    "\t\t#Retrieve Sensor Data For Day x\n",
    "        sensor_row = data[day][sensor_id]\n",
    "        \n",
    "        #Add the Sol ID to the row\n",
    "        sensor_row[\"sol_id\"] = sol_id\n",
    "        \n",
    "        sensor_rows.append(sensor_row)\n",
    "\n",
    "    return sensor_rows, sensor_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sol_id      av      mn      mx      ct\n",
      "0       0 -60.662 -95.821 -15.826  309399\n",
      "1       1 -62.760 -95.959  -8.545  351830\n",
      "2       2 -67.455 -96.828  -6.506  223227\n",
      "3       3 -62.386 -96.654  -9.915  177556\n",
      "4       4 -57.602 -96.011  -7.439  259074\n",
      "5       5 -63.280 -96.872 -15.908  248874\n",
      "6       6 -66.826 -96.912 -16.499  181986\n",
      "   sol_id     av     mn      mx      ct\n",
      "0       0  7.915  0.527  23.077  153433\n",
      "1       1  5.678  0.246  19.108  171610\n",
      "2       2  4.741  0.214  15.995  106953\n",
      "3       3  5.636  0.191  18.862   86796\n",
      "4       4  5.422  0.235  18.469  126068\n",
      "5       5  7.295  1.051  22.455  123240\n",
      "6       6  8.853  1.110  26.905   90466\n",
      "   sol_id       av        mn        mx      ct\n",
      "0       0  750.909  723.9181  771.5430  152448\n",
      "1       1  748.289  722.8493  764.1114  150704\n",
      "2       2  749.078  723.3106  766.2122  156607\n",
      "3       3  747.691  721.0958  766.4046   88773\n",
      "4       4  746.625  722.3939  764.9635  129594\n",
      "5       5  751.467  722.7706  768.7766  123307\n",
      "6       6  750.344  722.5664  767.1274   90985\n"
     ]
    }
   ],
   "source": [
    "#Atmospheric Temperature\n",
    "table_id = get_tbl_id(Table.Temp)\n",
    "atm_temp_rows, atm_temp_cols = structure_sensor_data(table_id,recent_sols)\n",
    "save_data(table_id, atm_temp_rows, atm_temp_cols)\n",
    "\n",
    "#Wind Speed\n",
    "table_id = get_tbl_id(Table.W_Speed)\n",
    "wspeed_rows, wspeed_cols = structure_sensor_data(table_id, recent_sols)\n",
    "save_data(table_id, wspeed_rows, wspeed_cols)\n",
    "\n",
    "#Atmospheric Pressure\n",
    "table_id = get_tbl_id(Table.Pressure)\n",
    "pre_rows, pre_cols = structure_sensor_data(table_id,recent_sols)\n",
    "save_data(table_id, pre_rows, pre_cols)\n",
    "\n",
    "#View Our Tables\n",
    "print(get_table(get_tbl_id(Table.Temp)))\n",
    "print(get_table(get_tbl_id(Table.W_Speed)))\n",
    "print(get_table(get_tbl_id(Table.Pressure)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we must structure our wind direction table.\n",
    "\n",
    "This table is a bit different, as we're working with directions of a compass which we refer to as the ordinals. These ordinals will be apart of our super key {sol_id,ordinal} for this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sol_id ordinal_id compass_point  compass_degrees  compass_right  \\\n",
      "0       0         11           WSW            247.5      -0.923880   \n",
      "1       0         10            SW            225.0      -0.707107   \n",
      "2       0         13           WNW            292.5      -0.923880   \n",
      "3       0         12             W            270.0      -1.000000   \n",
      "4       0         15           NNW            337.5      -0.382683   \n",
      "5       0         14            NW            315.0      -0.707107   \n",
      "6       0          1           NNE             22.5       0.382683   \n",
      "7       0          0             N              0.0       0.000000   \n",
      "8       0          5           ESE            112.5       0.923880   \n",
      "9       0          9           SSW            202.5      -0.382683   \n",
      "\n",
      "   compass_up     ct  \n",
      "0   -0.382683   6120  \n",
      "1   -0.707107   9572  \n",
      "2    0.382683  57611  \n",
      "3    0.000000  40009  \n",
      "4    0.923880   5026  \n",
      "5    0.707107  33779  \n",
      "6    0.923880     34  \n",
      "7    1.000000   1268  \n",
      "8   -0.382683      2  \n",
      "9   -0.923880     12  \n"
     ]
    }
   ],
   "source": [
    "#Wind Direction: Ordinals\n",
    "#Relation Key: {sol_id, ordinal_id}\n",
    "\n",
    "table_id = get_tbl_id(Table.W_Dir)\n",
    "\n",
    "wd_rows = []\n",
    "\n",
    "\"\"\"\n",
    "The columns for this table will be\n",
    "very different to the other tables.\n",
    "\"\"\"\n",
    "wd_cols = [\"sol_id\", \"ordinal_id\", \"compass_point\", \"compass_degrees\", \n",
    "\"compass_right\", \"compass_up\", \"ct\"]\n",
    "\n",
    "for sol in recent_sols:\n",
    "    day = id_to_sol(sol)\n",
    "\n",
    "    #Get all of our compass data for that day \n",
    "    ds = data[day][\"WD\"]\n",
    "    ordinals = ds.keys()\n",
    "\n",
    "    #For each direction on a wind rose\n",
    "    for ordinal in ordinals:\n",
    "        #Row for Day x, Ordinal y\n",
    "        next_row = ds[ordinal]\n",
    "        \n",
    "        #Super Key: { sol_id, ordinal_id }\n",
    "        next_row[\"sol_id\"] = sol\n",
    "        next_row[\"ordinal_id\"] = ordinal\n",
    "\n",
    "        wd_rows.append(next_row)\n",
    "\n",
    "save_data(table_id, wd_rows, wd_cols)\n",
    "\n",
    "#Print the first 10 entries in our table\n",
    "print(get_table(get_tbl_id(Table.W_Dir))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Of Data Pre-Processing & Integration\n",
    "\n",
    "This concludes the numerous stages of pre-processing and integration with the NASA InSight lander data.\n",
    "\n",
    "One of the first things that I learned studying data science was the importance of this step. \n",
    "You could say that skimping on this stage of data analysis is like entering a fight without having gone to the gym. You might be able to swing around a bit, but the potential for great work is limited.\n",
    "\n",
    "Maybe NetSoc & UCD Boxing should collaborate. Anyway...\n",
    "\n",
    "Time to gain some actual insights from our data. We've seen some numbers floating (excuse the pun) about, but haven't told the story the data provides for us. \n",
    "\n",
    "This next stage of data analysis is worth 40 percent of this assignment, and thus must be thorough. I'm going to ask five questions, and give you five answers. In between each, I'm going to see what tale the data can weave, and hopefully they give us insights into the story of Mars. At the core of each question, I would like to find a tangible insight. Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
